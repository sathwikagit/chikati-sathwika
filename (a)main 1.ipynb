{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for Social Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ch.sathwika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch dataset and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data, u_items_list, u_user_list, u_users_items_list, i_users_list):\n",
    "        self.data = data\n",
    "        self.u_items_list = u_items_list\n",
    "        self.u_users_list = u_user_list\n",
    "        self.u_users_items_list = u_users_items_list\n",
    "        self.i_users_list = i_users_list\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        uid = self.data[index][0]\n",
    "        iid = self.data[index][1]\n",
    "        label = self.data[index][2]\n",
    "        u_items = self.u_items_list[uid]\n",
    "        u_users = self.u_users_list[uid]\n",
    "        u_users_items = self.u_users_items_list[uid]\n",
    "        i_users = self.i_users_list[iid]\n",
    "\n",
    "        return (uid, iid, label), u_items, u_users, u_users_items, i_users\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_len = 45\n",
    "\n",
    "def collate_fn(batch_data):\n",
    "\n",
    "    uids, iids, labels = [], [], []\n",
    "    u_items, u_users, u_users_items, i_users = [], [], [], []\n",
    "    u_items_len, u_users_len, i_users_len = [], [], []\n",
    "\n",
    "    for data, u_items_u, u_users_u, u_users_items_u, i_users_i in batch_data:\n",
    "\n",
    "        (uid, iid, label) = data\n",
    "        uids.append(uid)\n",
    "        iids.append(iid)\n",
    "        labels.append(label)\n",
    "\n",
    "        # user-items\n",
    "        if len(u_items_u) <= truncate_len:\n",
    "            u_items.append(u_items_u)\n",
    "        else:\n",
    "            u_items.append(random.sample(u_items_u, truncate_len))\n",
    "        u_items_len.append(min(len(u_items_u), truncate_len))\n",
    "        \n",
    "        # user-users and user-users-items\n",
    "        if len(u_users_u) <= truncate_len:\n",
    "            u_users.append(u_users_u)\n",
    "            u_u_items = [] \n",
    "            for uui in u_users_items_u:\n",
    "                if len(uui) < truncate_len:\n",
    "                    u_u_items.append(uui)\n",
    "                else:\n",
    "                    u_u_items.append(random.sample(uui, truncate_len))\n",
    "            u_users_items.append(u_u_items)\n",
    "        else:\n",
    "            sample_index = random.sample(list(range(len(u_users_u))), truncate_len)\n",
    "            u_users.append([u_users_u[si] for si in sample_index])\n",
    "\n",
    "            u_users_items_u_tr = [u_users_items_u[si] for si in sample_index]\n",
    "            u_u_items = [] \n",
    "            for uui in u_users_items_u_tr:\n",
    "                if len(uui) < truncate_len:\n",
    "                    u_u_items.append(uui)\n",
    "                else:\n",
    "                    u_u_items.append(random.sample(uui, truncate_len))\n",
    "            u_users_items.append(u_u_items)\n",
    "\n",
    "        u_users_len.append(min(len(u_users_u), truncate_len))\t\n",
    "\n",
    "        # item-users\n",
    "        if len(i_users_i) <= truncate_len:\n",
    "            i_users.append(i_users_i)\n",
    "        else:\n",
    "            i_users.append(random.sample(i_users_i, truncate_len))\n",
    "        i_users_len.append(min(len(i_users_i), truncate_len))\n",
    "\n",
    "    batch_size = len(batch_data)\n",
    "\n",
    "    # padding\n",
    "    u_items_maxlen = max(u_items_len)\n",
    "    u_users_maxlen = max(u_users_len)\n",
    "    i_users_maxlen = max(i_users_len)\n",
    "    \n",
    "    u_item_pad = torch.zeros([batch_size, u_items_maxlen, 2], dtype=torch.long)\n",
    "    for i, ui in enumerate(u_items):\n",
    "        u_item_pad[i, :len(ui), :] = torch.LongTensor(ui)\n",
    "    \n",
    "    u_user_pad = torch.zeros([batch_size, u_users_maxlen], dtype=torch.long)\n",
    "    for i, uu in enumerate(u_users):\n",
    "        u_user_pad[i, :len(uu)] = torch.LongTensor(uu)\n",
    "    \n",
    "    u_user_item_pad = torch.zeros([batch_size, u_users_maxlen, u_items_maxlen, 2], dtype=torch.long)\n",
    "    for i, uu_items in enumerate(u_users_items):\n",
    "        for j, ui in enumerate(uu_items):\n",
    "            u_user_item_pad[i, j, :len(ui), :] = torch.LongTensor(ui)\n",
    "\n",
    "    i_user_pad = torch.zeros([batch_size, i_users_maxlen, 2], dtype=torch.long)\n",
    "    for i, iu in enumerate(i_users):\n",
    "        i_user_pad[i, :len(iu), :] = torch.LongTensor(iu)\n",
    "\n",
    "    uids = torch.LongTensor(uids)\n",
    "    iids = torch.LongTensor(iids)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    return uids, iids, labels, u_item_pad, u_user_pad, u_user_item_pad, i_user_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim//2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim//2, output_dim, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class UserModel(nn.Module):\n",
    "    def __init__(self, emb_dim, user_emb, item_emb, rating_emb):\n",
    "        super(UserModel, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.user_emb = user_emb\n",
    "        self.item_emb = item_emb\n",
    "        self.rating_emb = rating_emb\n",
    "\n",
    "        self.g_v = MLP(2*self.emb_dim, self.emb_dim)\n",
    "        \n",
    "        self.user_item_attn = MLP(2*self.emb_dim, 1)\n",
    "        self.aggr_items = Aggregator(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.user_user_attn = MLP(2*self.emb_dim, 1)\n",
    "        self.aggr_neighbors = Aggregator(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*self.emb_dim, self.emb_dim, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_dim, self.emb_dim, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_dim, self.emb_dim, bias = True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def forward(self, uids, u_item_pad, u_user_pad, u_user_item_pad):\n",
    "\n",
    "        q_a = self.item_emb(u_item_pad[:,:,0])\n",
    "        u_item_er = self.rating_emb(u_item_pad[:,:,1])\n",
    "        x_ia = self.g_v(torch.cat([q_a, u_item_er], dim=2).view(-1, 2*self.emb_dim)).view(q_a.size())\n",
    "        mask_u = torch.where(u_item_pad[:,:,0]>0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
    "        p_i = mask_u.unsqueeze(2).expand_as(x_ia) * self.user_emb(uids).unsqueeze(1).expand_as(x_ia)\n",
    "        alpha = self.user_item_attn(torch.cat([x_ia, p_i], dim=2).view(-1, 2*self.emb_dim)).view(mask_u.size())\n",
    "        alpha = torch.exp(alpha)*mask_u\n",
    "        alpha = alpha / (torch.sum(alpha, 1).unsqueeze(1).expand_as(alpha) + self.eps)\n",
    "        h_iI = self.aggr_items(torch.sum(alpha.unsqueeze(2).expand_as(x_ia) * x_ia, 1))\n",
    "\n",
    "\n",
    "        q_a_s = self.item_emb(u_user_item_pad[:,:,:,0])\n",
    "        u_user_item_er = self.rating_emb(u_user_item_pad[:,:,:,1])\n",
    "        x_ia_s = self.g_v(torch.cat([q_a_s, u_user_item_er], dim=2).view(-1, 2*self.emb_dim)).view(q_a_s.size())\n",
    "        mask_s = torch.where(u_user_item_pad[:,:,:,0]>0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
    "        p_i_s = mask_s.unsqueeze(3).expand_as(x_ia_s) * self.user_emb(u_user_pad).unsqueeze(2).expand_as(x_ia_s)\n",
    "        alpha_s = self.user_item_attn(torch.cat([x_ia_s, p_i_s], dim=3).view(-1, 2*self.emb_dim)).view(mask_s.size())\n",
    "        alpha_s = torch.exp(alpha_s)*mask_s\n",
    "        alpha_s = alpha_s / (torch.sum(alpha_s, 2).unsqueeze(2).expand_as(alpha_s) + self.eps)\n",
    "        h_oI_temp = torch.sum(alpha_s.unsqueeze(3).expand_as(x_ia_s) * x_ia_s, 2)\n",
    "        h_oI = self.aggr_items(h_oI_temp.view(-1, self.emb_dim)).view(h_oI_temp.size())\n",
    "        \n",
    "        beta = self.user_user_attn(torch.cat([h_oI, self.user_emb(u_user_pad)], dim = 2).view(-1, 2 * self.emb_dim)).view(u_user_pad.size())\n",
    "        mask_su = torch.where(u_user_pad > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
    "        beta = torch.exp(beta) * mask_su\n",
    "        beta = beta / (torch.sum(beta, 1).unsqueeze(1).expand_as(beta) + self.eps)\n",
    "        h_iS = self.aggr_neighbors(torch.sum(beta.unsqueeze(2).expand_as(h_oI) * h_oI, 1))\n",
    "\n",
    "        h_i = self.mlp(torch.cat([h_iI, h_iS], dim = 1))\n",
    "\n",
    "        return h_i\n",
    "\n",
    "\n",
    "class ItemModel(nn.Module):\n",
    "    def __init__(self, emb_dim, user_emb, item_emb, rating_emb):\n",
    "        super(ItemModel, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.user_emb = user_emb\n",
    "        self.item_emb = item_emb\n",
    "        self.rating_emb = rating_emb\n",
    "\n",
    "        self.g_u = MLP(2*self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.item_users_attn = MLP(2*self.emb_dim, 1)\n",
    "        self.aggr_users = Aggregator(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.eps = 1e-10\n",
    "    \n",
    "    def forward(self, iids, i_user_pad):\n",
    "\n",
    "        p_t = self.user_emb(i_user_pad[:,:,0])\n",
    "        i_user_er = self.rating_emb(i_user_pad[:,:,1])\n",
    "        mask_i = torch.where(i_user_pad[:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
    "        f_jt = self.g_u(torch.cat([p_t, i_user_er], dim = 2).view(-1, 2 * self.emb_dim)).view(p_t.size())\n",
    "        q_j = mask_i.unsqueeze(2).expand_as(f_jt) * self.item_emb(iids).unsqueeze(1).expand_as(f_jt)\n",
    "        mu_jt = self.item_users_attn(torch.cat([f_jt, q_j], dim = 2).view(-1, 2 * self.emb_dim)).view(mask_i.size())\n",
    "        mu_jt = torch.exp(mu_jt) * mask_i\n",
    "        mu_jt = mu_jt / (torch.sum(mu_jt, 1).unsqueeze(1).expand_as(mu_jt) + self.eps)\n",
    "        \n",
    "        z_j = self.aggr_users(torch.sum(mu_jt.unsqueeze(2).expand_as(f_jt) * f_jt, 1))\n",
    "\n",
    "        return z_j\n",
    "        \n",
    "    \n",
    "class GraphRec(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_ratings, emb_dim = 64):\n",
    "        super(GraphRec, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_ratings = n_ratings\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.user_emb = nn.Embedding(self.n_users, self.emb_dim, padding_idx=0)\n",
    "        self.item_emb = nn.Embedding(self.n_items, self.emb_dim, padding_idx=0)\n",
    "        self.rating_emb = nn.Embedding(self.n_ratings, self.emb_dim, padding_idx=0)\n",
    "\n",
    "        self.user_model = UserModel(self.emb_dim, self.user_emb, self.item_emb, self.rating_emb)\n",
    "        self.item_model = ItemModel(self.emb_dim, self.user_emb, self.item_emb, self.rating_emb)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*self.emb_dim, self.emb_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_dim, self.emb_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, uids, iids, u_item_pad, u_user_pad, u_user_item_pad, i_user_pad):\n",
    "\n",
    "        h_i = self.user_model(uids, u_item_pad, u_user_pad, u_user_item_pad)\n",
    "        z_j = self.item_model(iids, i_user_pad)\n",
    "\n",
    "        r_ij = self.mlp(torch.cat([h_i, z_j], dim=1))\n",
    "\n",
    "        return r_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device - cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device - ' + str(device))\n",
    "batch_size = 128\n",
    "embed_dim = 64\n",
    "learning_rate = 0.001\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset and preprocess it to form batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset_epinions.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "    valid_set = pickle.load(f)\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "with open('data/list_epinions.pkl', 'rb') as f:\n",
    "    u_items_list = pickle.load(f)\n",
    "    u_users_list = pickle.load(f)\n",
    "    u_users_items_list = pickle.load(f)\n",
    "    i_users_list = pickle.load(f)\n",
    "    (user_count, item_count, rate_count) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = GraphDataset(train_set, u_items_list, u_users_list, u_users_items_list, i_users_list)\n",
    "valid_data = GraphDataset(valid_set, u_items_list, u_users_list, u_users_items_list, i_users_list)\n",
    "test_data = GraphDataset(test_set, u_items_list, u_users_list, u_users_items_list, i_users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13451, 1638, 2)\n",
      "[(27001, 17), (821, 9), (17251, 2), (1513, 21), (4934, 2), (208191, 17), (1938, 2), (208193, 15), (89225, 20), (2961, 9), (33509, 6), (43570, 2), (11375, 2), (9539, 9), (1716, 3), (153072, 16), (575, 7), (537, 9), (1709, 2), (17890, 3), (46356, 17), (5817, 1), (160608, 12), (6163, 1), (943, 2), (17806, 10), (208190, 17), (1693, 2), (11244, 9), (9543, 7), (15952, 2), (1638, 2), (197, 7), (17295, 2), (70669, 15), (508, 1), (26983, 7), (5395, 7), (35772, 10), (37609, 2), (43454, 10), (16042, 2), (208192, 8), (106876, 9), (32671, 10), (9806, 11), (27395, 2), (72548, 2), (901, 2)]\n",
      "[11554, 19189, 12895, 2760, 16122, 16236, 8885, 15372, 15002, 603]\n",
      "[[(1338, 10), (61752, 5), (6058, 3), (189476, 5), (53513, 5), (52116, 5), (14579, 5), (11851, 5), (1668, 5), (10765, 3)], [(261394, 5), (56864, 5), (45475, 3), (4660, 3), (6044, 3), (2525, 3), (12822, 3), (8050, 3), (8319, 3), (115407, 5), (270384, 5), (3919, 3), (205361, 3), (64, 5), (87482, 3), (9557, 3), (128150, 5), (207296, 5), (3810, 3)], [(202523, 1), (202531, 8), (11267, 19), (36231, 19), (42950, 19), (76413, 19), (202524, 3), (6822, 19), (125344, 19), (202525, 18), (202529, 1), (202528, 11), (202527, 1)], [(47516, 3), (4091, 11), (67348, 21), (1239, 1), (66140, 11), (66869, 4), (66260, 4), (67830, 4), (66493, 3), (64951, 11), (53032, 4), (67321, 9), (66857, 4), (66867, 12), (66881, 4), (49138, 3), (67059, 6), (65263, 11), (16113, 19), (66193, 18), (67151, 1), (66921, 22), (3775, 1), (34231, 19), (66592, 19), (67144, 4), (66200, 5), (66242, 8), (67345, 5), (62972, 7), (21815, 1), (51674, 8), (67805, 11), (66467, 4), (67160, 3), (66876, 4), (9968, 3), (8970, 1), (21678, 3), (66247, 19), (61883, 8), (60938, 17), (66002, 11), (66595, 18), (48258, 4), (67039, 7), (65960, 11), (66568, 4), (17547, 22), (66463, 4), (65500, 11), (66772, 11), (66500, 18), (67535, 6), (67565, 17), (67795, 11), (67406, 1), (67808, 4), (11882, 21), (65164, 11), (14729, 3), (36978, 3), (66483, 11), (67137, 19), (45687, 1), (67801, 4), (34337, 1), (65391, 11), (67219, 8), (67698, 4), (54868, 1), (28420, 18), (67577, 3), (66775, 4), (50662, 11), (65933, 11), (13667, 22), (67200, 8), (67738, 10), (66805, 11), (61590, 12), (14007, 12), (18087, 12), (67514, 6), (39837, 3), (67806, 4), (1097, 1), (67303, 11), (1863, 12), (37562, 9), (67019, 5), (66625, 4), (66582, 11), (67070, 8), (67765, 4), (66295, 4), (66969, 11), (40794, 18), (66670, 4), (45689, 1), (66475, 12), (328, 12), (67038, 6), (67588, 4), (66747, 11), (67627, 1), (67150, 8), (65568, 11), (49775, 1), (66388, 21), (62809, 4), (33497, 6), (66401, 11), (66818, 12), (1081, 1), (66220, 11), (67504, 4), (67252, 11), (50701, 18), (10274, 19), (67664, 11), (13305, 3), (66553, 21), (67405, 12), (67166, 12), (3692, 19), (66913, 8), (67707, 4), (64655, 4), (41900, 11), (66559, 19), (34066, 10), (56645, 4), (66976, 10), (66742, 11), (66540, 11), (67352, 11), (67402, 6), (66831, 21), (67277, 5), (67036, 19), (8015, 1), (215, 12), (12370, 19), (67388, 11), (66961, 12), (1232, 1), (32957, 6), (66037, 11), (8692, 8), (66292, 11), (65091, 11), (45627, 1), (16585, 10), (1137, 1), (66226, 11), (6234, 11), (67544, 22), (66719, 4), (7090, 3), (67332, 11), (32899, 6), (67487, 17), (66700, 11), (9299, 6), (66147, 11), (3268, 1), (46781, 8), (42489, 19), (57764, 1), (3230, 11), (66198, 1), (66221, 19), (66827, 18), (23778, 12), (67427, 4), (39482, 4), (66585, 4), (66477, 7), (67785, 4), (6009, 11), (66405, 4), (66908, 18), (67810, 11), (67495, 5), (66714, 1), (22256, 11), (66275, 8), (66773, 4), (66258, 18), (1283, 1), (41105, 1), (34995, 1), (66962, 18), (41901, 11), (65573, 11), (66043, 11), (67603, 6), (66187, 19), (67229, 19), (66179, 11), (66518, 11), (41576, 10), (15903, 12), (3823, 11), (46426, 12), (67414, 12), (66820, 7), (26260, 6), (67582, 6), (8708, 11), (9036, 11), (67772, 4), (67630, 4), (66502, 3), (67250, 4), (67516, 5), (66917, 21), (66762, 4), (24571, 3), (44918, 1), (7137, 6), (67094, 1), (66283, 19), (50014, 12), (66162, 5), (67236, 12), (66149, 11), (67266, 11), (66649, 4), (66436, 21), (66333, 12), (67192, 11), (2388, 11), (67360, 4), (66930, 4), (66214, 11), (66459, 4), (3529, 6), (5288, 11), (36057, 19), (67571, 10), (51422, 19), (22255, 11), (7894, 11), (66527, 12), (67336, 6), (23789, 12), (66566, 4), (19100, 11), (29371, 12), (1100, 1), (66514, 4), (67752, 4), (66640, 4), (67221, 12), (66158, 11), (67216, 8), (66702, 4), (57356, 12), (18666, 3), (60277, 6), (67358, 4), (67385, 5), (50112, 12), (1147, 1), (66889, 11), (67575, 4), (29369, 12), (67140, 8), (67124, 8), (66626, 4), (66683, 8), (1123, 12), (67702, 1), (67542, 22), (2640, 8), (51799, 11), (66201, 4), (62908, 15), (66148, 11), (67097, 18), (66157, 4), (66234, 18), (66957, 12), (66803, 11), (66890, 3), (66931, 5), (67646, 4), (43030, 22), (66910, 8), (66794, 11), (66508, 4), (66290, 4), (29057, 3), (66562, 21), (67475, 3), (46795, 3), (33366, 15), (66824, 18), (66993, 21), (3851, 11), (67500, 5), (67381, 12), (54060, 12), (65922, 11), (50454, 8), (1178, 1), (10247, 1), (66593, 4), (67610, 1), (66911, 11), (66704, 12), (611, 11), (1082, 3), (67634, 4), (65766, 11), (1094, 1), (43126, 18), (66171, 6), (46816, 11), (56679, 12), (19448, 11), (67640, 10), (66610, 4), (66863, 4), (66713, 4), (66232, 5), (66130, 8), (65295, 11), (67340, 11), (67847, 4), (2311, 11), (65805, 11), (67274, 11), (67758, 4), (41935, 11), (67152, 3), (67613, 10), (43028, 1), (66364, 4), (5924, 6), (66966, 1), (2515, 11), (67567, 10), (1911, 11), (29050, 3), (38691, 12), (67310, 4), (66195, 11), (20446, 11), (67520, 10), (66484, 12), (67465, 11), (66256, 19), (22272, 11), (66756, 11), (66419, 8), (60638, 8), (12321, 11), (6896, 11), (66209, 18), (59261, 1), (67473, 3), (22250, 11), (67014, 4), (67281, 6), (54023, 12), (66457, 11), (65972, 11), (67155, 12), (22248, 11), (8242, 3), (66496, 21), (66359, 19), (64760, 6), (50792, 8), (66278, 11), (65138, 11), (67492, 12), (378, 6), (67056, 8), (67583, 6), (3809, 1), (7424, 1), (61238, 19), (51426, 19), (37597, 11), (67141, 4), (67279, 3), (67694, 4), (66905, 4), (66615, 11), (67743, 11), (66135, 10), (67247, 22), (66394, 8), (66263, 10), (66571, 11), (67423, 3), (67713, 4), (66621, 8), (67082, 18), (4733, 11), (66990, 12), (12945, 10), (19687, 11), (51050, 4), (59912, 4), (2125, 11), (67599, 6), (67793, 1), (303, 10), (118, 11), (21840, 10), (2531, 11), (67734, 4), (183, 1), (66205, 21), (66664, 4), (66504, 12), (67556, 4), (19165, 4), (64791, 11), (6363, 19), (66912, 5), (27801, 1), (66579, 19), (67075, 16), (67461, 4), (25713, 11), (67390, 4), (66537, 18), (67657, 1), (67462, 10), (66409, 10), (66567, 4), (1042, 8), (66231, 4), (67204, 12), (67679, 11), (67687, 4), (66472, 11), (67714, 1), (8544, 1), (266, 6), (67675, 4), (3166, 1), (66808, 11), (66740, 11), (45113, 4), (40337, 12), (65603, 11), (5455, 12), (39169, 3), (57993, 4), (40248, 3), (3429, 19), (66318, 11), (59876, 11), (67025, 4), (67642, 10), (66708, 4), (23559, 3), (26609, 19), (66503, 4), (66916, 8), (66785, 4), (45154, 10), (19685, 11), (23994, 10), (66611, 17), (66375, 11), (45135, 3), (66430, 6), (3362, 12), (67444, 8), (66470, 4), (66335, 11), (67430, 6), (66774, 12), (55702, 11), (61243, 8), (67798, 11), (67771, 4), (67121, 11), (65735, 11), (67435, 11), (66698, 4), (14575, 11), (67253, 12), (67770, 4), (66797, 4), (67667, 4), (66326, 11), (64985, 11), (12243, 6), (43706, 4), (57357, 12), (19193, 18), (67275, 11), (3164, 18), (43193, 1), (66323, 8), (66141, 11), (67746, 10), (8782, 11), (66638, 11), (67033, 11), (66555, 12), (46596, 11), (66168, 11), (67481, 4), (65494, 11), (66515, 8), (66854, 1), (66648, 8), (19211, 18), (66641, 11), (67422, 6), (66583, 6), (43657, 5), (1285, 1), (10027, 7), (67263, 11), (66386, 18), (66243, 8), (67157, 3), (50175, 11), (9206, 1), (41161, 12), (66622, 6), (601, 10), (66199, 4), (1384, 12), (66146, 5), (66556, 18), (11150, 12), (66620, 4), (67248, 22), (66685, 4), (9169, 3), (53337, 5), (67276, 3), (67683, 4), (66482, 4), (66744, 11), (21421, 12), (67591, 4), (67562, 6), (67364, 4), (62849, 19), (47589, 3), (66980, 8), (66681, 4), (66181, 4), (67232, 1), (67671, 4), (67651, 10), (7723, 3), (67107, 3), (67223, 8), (34660, 1), (4379, 11), (66465, 4), (66325, 4), (1119, 1), (57700, 1), (8024, 11), (67454, 11), (66524, 4), (66743, 18), (66636, 4), (66279, 4), (8740, 6), (39151, 18), (65699, 11), (67189, 4), (67609, 11), (65554, 11), (53411, 8), (12656, 22), (4095, 11), (66183, 4), (887, 10), (66188, 7), (9557, 3), (59350, 15), (67182, 11), (67041, 19), (12826, 3), (67329, 10), (57702, 1), (50948, 1), (22283, 11), (3828, 11), (66412, 4), (66830, 18), (65897, 11), (66499, 11), (12856, 3), (66251, 7), (66946, 5), (52024, 8), (67269, 11), (67117, 3), (67280, 4), (67656, 1), (67418, 10), (2229, 11), (1246, 1), (52654, 12), (67290, 11), (66919, 4), (12911, 3), (67130, 1), (65163, 11), (53534, 10), (35761, 1), (67739, 4), (66727, 11), (66852, 8), (66768, 4), (66780, 11), (14419, 3), (10238, 11), (47530, 3), (66361, 11), (65159, 11), (67496, 11), (67369, 11), (67322, 22), (28140, 4), (67849, 11), (1049, 1), (11137, 1), (22312, 11), (66501, 4), (66855, 11), (66751, 12), (66972, 7), (65572, 11), (66802, 4), (67171, 4), (57350, 6), (53325, 12), (31061, 1), (26015, 19), (2385, 11), (66619, 5), (66963, 11), (836, 3), (66334, 18), (65006, 11), (66709, 11), (67453, 11), (6325, 11), (67246, 12), (65591, 11), (66596, 19), (67451, 5), (67301, 3), (22447, 22), (66377, 1), (66139, 3), (66444, 4), (66471, 1), (1223, 1), (66985, 4), (32206, 10), (65804, 11), (52, 11), (53288, 5), (66789, 7), (10994, 6), (67424, 11), (8753, 11), (66968, 12), (2448, 11), (56, 11), (16099, 11), (67234, 21), (43658, 5), (28095, 12), (67463, 3), (4994, 8), (3380, 10), (66391, 4), (47444, 3), (66618, 10), (66953, 4), (66255, 22), (31260, 19), (23374, 19), (155, 3), (66363, 11), (66929, 5), (59675, 11), (66574, 4), (66541, 4), (11775, 12), (67733, 10), (29027, 10), (32699, 8), (10343, 1), (5299, 3), (66288, 19), (66850, 18), (2452, 11), (66450, 8), (66684, 11), (5722, 1), (2474, 11), (67821, 4), (66729, 11), (2496, 11), (66599, 4), (38016, 4), (66213, 11), (43182, 10), (3790, 3), (67647, 10), (66925, 11), (67551, 5), (65945, 11), (1105, 1), (67540, 4), (67532, 10), (67769, 4), (49412, 1), (66038, 11), (1338, 10), (66906, 11), (7651, 10), (66265, 21), (66169, 1), (16495, 11), (3161, 8), (67330, 11), (816, 11), (66779, 12), (19270, 7), (31057, 21), (66204, 11), (31402, 10), (66441, 18), (67736, 10), (67212, 8), (66679, 4), (67032, 11), (50311, 11), (59371, 6), (50119, 11), (64835, 11), (66748, 4), (66750, 11), (67378, 1), (21260, 19), (8497, 3), (65542, 11), (55438, 11), (51794, 11), (65292, 11), (67549, 3), (47573, 3), (59151, 11), (43279, 11), (22267, 11), (67057, 4), (66942, 1), (65604, 11), (67697, 10), (1237, 1), (67474, 3), (66357, 4), (45650, 1), (65279, 11), (67635, 4), (67392, 18), (26031, 22), (67268, 17), (66722, 4), (67356, 3), (66413, 12), (3020, 12), (66547, 4), (66996, 18), (3222, 11), (67827, 11), (67470, 3), (1388, 11), (4890, 12), (67262, 4), (65085, 11), (66480, 4), (23180, 12), (28860, 8), (2527, 11), (65878, 11), (66466, 4), (41471, 3), (67528, 4), (66812, 11), (49775, 6), (67566, 11), (67239, 1), (32660, 19), (444, 10), (66190, 19), (65445, 11), (67835, 11), (66282, 21), (59859, 3), (34381, 4), (67164, 12), (66558, 7), (50031, 12), (32222, 18), (66939, 10), (22258, 12), (67817, 1), (66979, 12), (66022, 11), (67436, 5), (66673, 18), (66328, 18), (66777, 4), (66250, 18), (66589, 11), (3772, 1), (65966, 11), (66399, 1), (67499, 11), (8035, 1), (1227, 1), (52070, 11), (66347, 8), (66312, 21), (15916, 1), (66137, 5), (62405, 12), (66688, 12), (15542, 8), (66617, 18), (64941, 11), (56115, 6), (2468, 11), (66828, 18), (67458, 4), (66532, 4), (66660, 1), (67049, 4), (7116, 22), (11042, 19), (66672, 4), (10808, 21), (2092, 11), (22299, 11), (67842, 11), (66654, 11), (67482, 5), (17854, 6), (37098, 12), (66845, 19), (66202, 11), (66291, 8), (2054, 11), (31463, 15), (66273, 4), (66155, 18), (33188, 1), (33030, 8), (14692, 11), (67271, 12), (67008, 12), (24151, 19), (1229, 1), (66447, 12), (67172, 11), (67626, 6), (22247, 11), (66332, 4), (379, 6), (12587, 11), (11030, 11), (67040, 21), (66940, 4), (67581, 11), (66659, 4), (66367, 4), (66474, 18), (67768, 4), (67109, 8), (67169, 6), (66705, 1), (5278, 6), (67174, 19), (67233, 10), (66406, 21), (67297, 4), (66254, 4), (369, 1), (50617, 21), (16748, 11), (2477, 11), (66971, 1), (64432, 18), (66018, 11), (66795, 11), (41675, 3), (66801, 11), (67100, 12), (12892, 3), (66934, 11), (67244, 10), (66739, 4), (41914, 11), (67570, 15), (4898, 1), (67311, 4), (5102, 11), (67240, 7), (67710, 4), (3223, 11), (67087, 4), (67085, 8), (67237, 11), (17488, 3), (66955, 12), (11986, 11), (34174, 12), (1322, 11), (5715, 1), (41465, 3), (2516, 11), (66668, 4), (329, 12), (66366, 4), (67067, 4), (65188, 11), (67243, 4), (67578, 3), (67029, 12), (66185, 18), (67600, 6), (50095, 12), (67370, 12), (54347, 1), (21862, 4), (66172, 5), (66587, 4), (19246, 18), (67586, 4), (535, 6), (33907, 11), (44807, 1), (47647, 3), (22689, 10), (33504, 6), (67779, 4), (67110, 16), (66507, 3), (66426, 12), (65559, 11), (10346, 3), (66731, 11), (66978, 8), (66989, 1), (66522, 4), (67081, 21), (65666, 11), (16106, 3), (39210, 10), (67652, 10), (66382, 4), (24008, 3), (50237, 19), (67398, 5), (66738, 11), (67165, 8), (2252, 5), (31300, 15), (9207, 11), (5769, 12), (4083, 11), (31538, 18), (47471, 3), (66486, 7), (8707, 11), (66356, 4), (66133, 4), (61367, 19), (66531, 10), (56528, 1), (67190, 11), (66861, 4), (67755, 10), (67042, 1), (697, 11), (67419, 6), (3190, 18), (67224, 9), (67554, 11), (67429, 4), (1788, 1), (41463, 3), (57028, 6), (67721, 4), (66569, 21), (67217, 17), (1916, 11), (66573, 4), (47556, 3), (65777, 11), (2406, 11), (4907, 1), (20027, 10), (56614, 3), (32563, 1), (66300, 10), (81, 11), (66951, 12), (28782, 1), (67834, 11), (21168, 3), (1198, 1), (35167, 1), (67708, 4), (48508, 19), (67510, 22), (33478, 11), (66960, 8), (67090, 19), (37654, 19), (66695, 4), (12045, 11), (67608, 4), (67735, 11), (66196, 3), (30573, 4), (66776, 4), (67283, 4), (64860, 11), (6315, 4), (67255, 11), (66760, 4), (12130, 10), (4252, 12), (16673, 4), (67655, 4), (66192, 18), (66995, 11), (374, 1), (12782, 3), (66153, 4), (67093, 12), (67439, 4), (67099, 10), (67300, 22), (13477, 3), (67426, 3), (65609, 11), (50017, 18), (1321, 11), (48298, 4), (1019, 1), (67716, 4), (17287, 18), (448, 8), (67602, 5), (67471, 4), (31076, 6), (22239, 11), (11895, 12), (66224, 11), (8784, 11), (2236, 11), (66865, 4), (45516, 1), (67545, 4), (66381, 8), (17559, 6), (10132, 3), (67058, 8), (67543, 1), (66982, 12), (473, 1), (8862, 3), (66825, 11), (67653, 1), (45605, 1), (67668, 4), (67184, 3), (67080, 4), (66342, 8), (67349, 12), (26675, 8), (40385, 6), (40946, 12), (15011, 11), (1224, 1), (66914, 8), (443, 11), (66994, 3), (62246, 7), (1082, 10), (67123, 12), (2420, 11), (41898, 11), (67180, 21), (32115, 17), (66442, 21), (41479, 11), (47650, 3), (11840, 4), (67623, 6), (66371, 11), (6441, 12), (67604, 4), (67242, 9), (41953, 11), (5709, 11), (53214, 18), (67368, 3), (27549, 4), (924, 24), (49527, 11), (17311, 11), (67122, 1), (4009, 6), (122, 1), (67625, 6), (36368, 19), (6792, 3), (67074, 21), (2485, 11), (13885, 18), (66455, 11), (67790, 4), (67028, 21), (2953, 19), (67644, 1), (67497, 4), (67068, 7), (52672, 12), (65067, 11), (67238, 1), (67777, 11), (66614, 11), (3231, 11), (66268, 4), (64040, 1), (50408, 12), (66766, 7), (67161, 19), (8712, 6), (23334, 5), (67312, 4), (307, 11), (66307, 12), (8242, 10), (13841, 22), (14433, 11), (2464, 11), (391, 11), (15064, 4), (67749, 1), (66826, 12), (67584, 16), (65883, 11), (66267, 18), (16651, 1), (66259, 19), (66239, 4), (39590, 4), (66787, 4), (67015, 1), (54172, 12), (12028, 11), (22240, 11), (67498, 19), (66983, 11), (65940, 11), (33988, 1), (67210, 9), (66843, 19), (66669, 12), (2652, 8), (66849, 4), (30194, 4), (12050, 11), (3185, 18), (67357, 12), (67479, 18), (67333, 11), (1110, 1), (33548, 11), (66757, 4), (67676, 10), (66408, 4), (66745, 11), (450, 11), (19727, 8), (2484, 11), (66935, 1), (40982, 22), (36641, 12), (67104, 4), (1579, 6), (67309, 3), (67004, 1), (1236, 1), (66519, 11), (66657, 12), (6331, 11), (67670, 4), (26965, 6), (65977, 11), (67214, 6), (66841, 1), (34100, 1), (66811, 18), (479, 10), (66165, 19), (21577, 3), (66456, 4), (1170, 1), (67618, 6), (22303, 4), (4474, 11), (66853, 4), (1171, 7), (2300, 11), (67483, 11), (67272, 11), (66898, 12), (59692, 11), (22298, 11), (67726, 4), (66440, 12), (27397, 3), (67624, 10), (28737, 19), (66674, 11), (26081, 9), (58257, 19), (67410, 11), (66671, 4), (66126, 12), (66701, 11), (66941, 4), (66788, 4), (50224, 21), (2481, 11), (66699, 11), (67147, 12), (67048, 11), (66240, 5), (35789, 11), (67173, 21), (22315, 11), (3407, 6), (5728, 6), (66438, 4), (66403, 4), (16967, 14), (66724, 4), (22287, 11), (66536, 1), (26529, 1), (17022, 19), (67002, 19), (67245, 21), (67111, 1), (66909, 12), (36495, 8), (66166, 19), (67648, 12), (67211, 7), (67763, 4), (67285, 3), (33505, 6), (20742, 8), (16398, 11), (67129, 1), (5150, 6), (66759, 4), (51033, 4), (1143, 1), (370, 10), (67209, 8), (67515, 4), (12044, 11), (31304, 1), (67185, 12), (64011, 22), (67616, 10), (66933, 4), (1519, 10), (66178, 11), (67843, 11), (66417, 10), (32551, 3), (66458, 19), (4772, 1), (64873, 11), (7579, 12), (66846, 11), (8997, 1), (66602, 10), (15973, 12), (66793, 12), (65034, 11), (67494, 5), (66692, 20), (4974, 1), (67383, 5), (32615, 6), (684, 11), (66938, 10), (5782, 10), (24962, 12), (67661, 4), (66294, 4), (67105, 18), (31970, 11), (66937, 12), (66903, 11), (51951, 11), (67132, 3), (66707, 7), (66783, 4), (66597, 4), (67065, 1), (47042, 1), (1872, 19), (20831, 8), (67010, 19), (66248, 11), (40759, 1), (66576, 4), (2469, 11), (65023, 11), (35775, 3), (66330, 11), (2517, 11), (50727, 21), (67460, 11), (67376, 12), (7618, 1), (4695, 11), (2983, 1), (67428, 10), (67673, 4), (51825, 1), (66832, 4), (17024, 18), (67289, 10), (61281, 18), (14576, 11), (66346, 4), (66784, 4), (52612, 18), (36364, 11), (66271, 19), (59815, 4), (67374, 8), (1023, 1), (8045, 3), (50659, 8), (67095, 18), (41882, 11), (66191, 19), (67227, 4), (67757, 4), (2424, 11), (67709, 11), (65625, 11), (67654, 1), (66550, 7), (67389, 15), (67552, 10), (67509, 7), (28253, 18), (67831, 11), (6346, 11), (67525, 22), (67522, 10), (67727, 4), (66796, 11), (67530, 10), (5492, 7), (66809, 1), (67560, 17), (66764, 19), (12027, 11), (66603, 4), (67823, 12), (31050, 11), (67011, 11), (65714, 11), (38803, 4), (66398, 4), (2479, 11), (62370, 10), (23163, 7), (67298, 11), (66327, 4), (6335, 11), (27073, 21), (64945, 11), (66338, 6), (67265, 11), (67815, 4), (66498, 4), (66725, 18), (66862, 4), (67295, 11), (23871, 19), (67555, 11), (67133, 5), (39714, 3), (67106, 18), (66609, 4), (67235, 11), (66624, 17), (17853, 6), (65247, 11), (67756, 4), (47455, 3), (67118, 12), (66549, 9), (45981, 1), (1269, 10), (51, 1), (11106, 8), (67162, 18), (64884, 11), (61726, 4), (2404, 11), (66212, 19), (64267, 21), (36892, 4), (67680, 6), (10913, 15), (67096, 1), (67350, 21), (66807, 11), (39380, 4), (67325, 11), (67021, 21), (67559, 10), (67034, 8), (10194, 12), (66868, 18), (67518, 22), (67507, 12), (66634, 21), (66997, 18), (50553, 18), (66138, 11), (67375, 12), (23870, 4), (67344, 8), (49633, 11), (25732, 3), (67112, 1), (31438, 11), (66580, 12), (41478, 3), (67078, 18), (65380, 11), (67701, 4), (67590, 4), (10805, 12), (22301, 11), (66184, 18), (66813, 11), (66400, 11), (66943, 12), (64953, 11), (14577, 11), (46411, 11), (67767, 4), (66675, 1), (67051, 19), (41944, 11), (63005, 11), (67409, 4), (66230, 11), (47559, 3), (66360, 11), (9279, 4), (66798, 11), (66257, 10), (66160, 11), (66170, 3), (67023, 18), (66385, 4), (66396, 10), (67331, 11), (41305, 6), (54201, 8), (66433, 22), (67446, 6), (67669, 4), (66665, 11), (66142, 11), (16633, 18), (14417, 10), (42716, 3), (66379, 12), (47743, 3), (67508, 4), (67452, 12), (67437, 3), (67838, 11), (16289, 12), (22306, 11), (38913, 8), (5898, 10), (51539, 18), (65051, 11), (66353, 18), (6782, 3), (67138, 4), (67802, 22), (59845, 3), (66790, 11), (67455, 4), (67119, 4), (35190, 16), (17975, 1), (66741, 8), (67116, 1), (9482, 3), (66984, 8), (60351, 8), (66697, 1), (66992, 4), (67434, 8), (67775, 4), (67728, 10), (66864, 18), (43575, 1), (3387, 11), (67447, 6), (1235, 1), (67299, 11), (64191, 19), (65858, 4), (16555, 1), (22279, 4), (1118, 1), (66299, 4), (22293, 11), (66159, 19), (67168, 3), (50320, 3), (66658, 11), (66746, 6), (67337, 11), (66418, 11), (66896, 11), (61473, 18), (16347, 1), (46014, 21), (15141, 3), (22297, 11), (66693, 4), (67548, 4), (66899, 11), (2408, 11), (26347, 10), (32305, 4), (66136, 4), (67629, 4), (65847, 11), (67030, 21), (16667, 1), (65616, 11), (67205, 17), (66816, 11), (67678, 1), (37617, 3), (66023, 11), (66229, 18), (66644, 11), (18836, 4), (2764, 8), (32443, 1), (6090, 3), (5390, 18), (61905, 1), (10480, 18), (10954, 18), (66331, 13), (10914, 12), (61652, 1), (11754, 3), (67203, 7), (67230, 12), (57042, 21), (3376, 19), (34529, 1), (67607, 4), (1261, 1), (24925, 1), (67538, 4), (442, 11), (1438, 15), (39472, 4), (5396, 3), (39518, 4), (65437, 11), (61297, 19), (66461, 4), (67685, 4), (66454, 18), (66392, 19), (11016, 11), (67637, 6), (67135, 1), (66167, 4), (10095, 11), (66888, 3), (65115, 11), (43685, 12), (67177, 11), (38840, 6), (4706, 11), (67594, 1), (45707, 4), (67633, 12), (59667, 11), (16499, 11), (66604, 12), (67101, 4), (28409, 8), (67154, 19), (66337, 4), (3400, 11), (67761, 11), (57131, 7), (66309, 4), (14390, 1), (66981, 12), (66612, 4), (66856, 11), (66393, 21), (58015, 4), (65708, 11), (46796, 15), (67784, 4), (66411, 4), (14003, 3), (67553, 12), (25572, 1), (12801, 3), (67084, 19), (66703, 4), (18821, 1), (66733, 3), (67592, 6), (11432, 11), (66319, 8), (51838, 4), (66715, 11), (66180, 18), (47499, 3), (61491, 19), (67672, 4), (66490, 4), (57213, 4), (66778, 11), (66653, 18), (67179, 12), (3386, 11), (66512, 4), (66544, 4), (67393, 5), (67813, 4), (66706, 4), (12098, 11), (16381, 11), (66473, 4), (22300, 11), (28990, 3), (1122, 12), (66782, 12), (22811, 11), (4087, 11), (3232, 11), (40932, 12), (66651, 4), (67064, 8), (22310, 11), (1038, 1), (50152, 12), (19095, 12), (21128, 10), (65687, 11), (67631, 1), (5693, 12), (67026, 18), (21222, 19), (66718, 4), (47524, 3), (66144, 21), (34060, 10), (67464, 4), (66629, 8), (67846, 11), (66879, 21), (67809, 4), (56943, 11), (18732, 3), (6329, 11), (22288, 11), (4062, 19), (66662, 4), (2732, 11), (9386, 1), (16532, 11), (67456, 18), (43664, 5), (1025, 18), (12909, 3), (24192, 1), (65426, 11), (59906, 11), (66633, 18), (66578, 11), (67432, 4), (29058, 3), (67612, 6), (36808, 12), (67477, 11), (17802, 4), (10495, 19), (66880, 11), (5885, 22), (4991, 19), (67505, 15), (2417, 11), (11931, 12), (50641, 1), (6472, 21), (9969, 3), (67318, 4), (67377, 11), (4267, 10), (18876, 19), (67433, 11), (67305, 8), (5574, 4), (66737, 1), (67593, 10), (39296, 4), (27824, 6), (66605, 1), (67326, 11), (67797, 11), (67719, 10), (66606, 12), (60281, 15), (32120, 19), (50425, 19), (66304, 1), (614, 11), (67126, 4), (67304, 10), (67191, 11), (66228, 5), (3680, 17), (66804, 6), (7113, 4), (53404, 4), (66581, 4), (60966, 19), (34311, 4), (66286, 18), (65078, 11), (66530, 18), (66645, 11), (66915, 11), (66449, 4), (11703, 3), (67622, 4), (67020, 3), (67597, 1), (67355, 4), (22294, 11), (67197, 8), (193, 10), (67788, 4), (24155, 11), (66952, 18), (67012, 12), (67288, 8), (53, 10), (57731, 1), (16105, 11), (61482, 18), (17718, 10), (66924, 7), (66429, 18), (45034, 15), (43276, 12), (67573, 3), (67732, 4), (12851, 3), (19988, 11), (65169, 11), (66542, 4), (67484, 4), (66509, 22), (51949, 11), (66922, 8), (66694, 11), (67145, 19), (67658, 11), (29773, 4), (7117, 12), (66351, 8), (67493, 5), (11041, 11), (66964, 1), (66528, 4), (66352, 4), (22289, 11), (16401, 11), (47138, 15), (5699, 1), (67338, 3), (938, 11), (16580, 11), (50231, 21), (66525, 18), (22286, 11), (66329, 18), (1151, 1), (188, 11), (40931, 11), (66792, 4), (51675, 10), (16545, 11), (57813, 1), (17633, 1), (41955, 11), (66238, 12), (4194, 4), (67257, 11), (67469, 5), (2488, 11), (67745, 4), (66545, 4), (8501, 1), (67845, 4), (66378, 12), (66131, 11), (67148, 19), (48852, 3), (66266, 11), (4359, 11), (67712, 1), (67208, 8), (67783, 15), (67489, 11), (66425, 4), (40615, 12), (65635, 11), (67365, 4), (2586, 6), (66607, 5), (67440, 4), (20011, 3), (10528, 1), (23133, 11), (66431, 4), (67674, 1), (67742, 4), (21464, 18), (66182, 11), (66897, 4), (65657, 11), (66736, 4), (67403, 4), (67258, 3), (66316, 4), (66539, 12), (67069, 5), (1249, 1), (47399, 3), (66817, 4), (66836, 4), (67098, 18), (67643, 6), (5890, 1), (66276, 11), (1578, 12), (67228, 1), (66372, 11), (67744, 4), (66904, 19), (56228, 7), (45604, 19), (22243, 11), (15700, 3), (66956, 8), (66835, 11), (3317, 10), (59368, 7), (65513, 11), (67415, 5), (66485, 4), (67628, 4), (12889, 6), (1208, 1), (27689, 6), (62818, 17), (67617, 1), (66754, 4), (67703, 1), (61269, 18), (66667, 4), (41933, 11), (62900, 15), (67693, 4), (65671, 11), (50147, 12), (66233, 11), (41963, 11), (67639, 10), (66927, 12), (11748, 4), (66143, 11), (66885, 19), (10476, 11), (66936, 12), (66219, 11), (66758, 1), (20068, 19), (40608, 3), (67195, 10), (66407, 10), (21427, 18), (67576, 3), (66646, 8), (2354, 11), (4092, 11), (67839, 11), (66535, 1), (2118, 11), (67691, 4), (7100, 4), (3604, 4), (61703, 19), (45879, 12), (66829, 4), (16785, 1), (67286, 4), (50261, 3), (1312, 11), (66959, 4), (67022, 18), (37494, 18), (66365, 4), (36639, 12), (67681, 11), (67729, 4), (66298, 18), (67824, 11), (1205, 1), (66661, 4), (10999, 8), (67050, 12), (66655, 4), (66156, 11), (67412, 4), (66886, 19), (67526, 22), (67317, 4), (66194, 5), (67660, 1), (33501, 6), (6243, 18), (67047, 12), (67319, 4), (38146, 8), (66223, 11), (43668, 4), (66344, 10), (65626, 11), (66902, 11), (66374, 19), (66730, 4), (16528, 11), (67541, 22), (66506, 22), (67529, 11), (37556, 21), (66305, 8), (17674, 19), (67027, 21), (67156, 12), (67071, 1), (67774, 10), (67416, 11), (3167, 1), (26063, 12), (12849, 3), (66734, 8), (66203, 4), (12984, 10), (16490, 11), (35094, 21), (6078, 1), (50436, 12), (67382, 4), (66516, 18), (67796, 1), (66487, 19), (67506, 5), (64971, 11), (66132, 18), (2512, 11), (67614, 6), (67490, 5), (259, 3), (22186, 11), (17031, 1), (17816, 12), (67339, 21), (65866, 11), (14119, 3), (67072, 11), (67307, 3), (47519, 3), (27948, 12), (22245, 11), (1216, 1), (6091, 3), (33480, 11), (67158, 4), (67328, 22), (46583, 3), (18647, 15), (66437, 4), (67812, 4), (34098, 1), (66163, 4), (12886, 3), (66324, 11), (45932, 15), (2975, 11), (67270, 8), (67536, 5), (67818, 1), (23783, 11), (616, 11), (7042, 3), (11028, 11), (67425, 3), (66973, 8), (8545, 1), (1209, 8), (2501, 11), (2285, 11), (67139, 21), (67684, 4), (45812, 15), (67764, 11), (32100, 22), (14006, 12), (66647, 11), (50078, 11), (66150, 11), (308, 11), (4649, 11), (45700, 1), (50406, 3), (4791, 11), (1290, 10), (66520, 10), (66586, 7), (36630, 12), (67108, 18), (63043, 15), (67055, 1), (38141, 12), (66844, 23), (61489, 8), (67073, 8), (9282, 19), (18007, 1), (66560, 4), (67814, 11), (66840, 11), (66322, 11), (66321, 4), (25730, 18), (54826, 1), (66476, 4), (49, 11), (66210, 20), (66608, 19), (66245, 4), (67183, 17), (66494, 19), (67089, 3), (45768, 16), (12698, 1), (1270, 1), (66907, 5), (55125, 11), (67799, 11), (64761, 6), (67828, 4), (66923, 5), (65619, 11), (32670, 11), (16481, 11), (66891, 15), (1570, 19), (67519, 22), (67320, 15), (66491, 4), (66977, 7), (66293, 11), (67511, 1), (66272, 19), (66380, 8), (66728, 11), (37629, 3), (66577, 4), (51165, 4), (66345, 22), (67115, 12), (2282, 11), (66354, 21), (3346, 1), (12874, 3), (44865, 19), (16444, 3), (67778, 11), (51250, 18), (67017, 3), (33959, 8), (67052, 15), (33477, 11), (62045, 19), (67215, 12), (66900, 4), (66769, 11), (7941, 10), (67373, 9), (67284, 11), (34443, 11), (888, 1), (67819, 1), (57365, 12), (67327, 3), (67750, 11), (66469, 11), (66874, 4), (66314, 4), (67468, 4), (47474, 3), (16519, 11), (3024, 12), (4327, 8), (66368, 4), (66358, 8), (67387, 5), (4701, 11), (65823, 11), (66225, 11), (16033, 3), (66264, 11), (67114, 1), (24054, 12), (49204, 4), (66478, 18), (28761, 8), (67397, 11), (10338, 1), (40953, 12), (66414, 10), (66007, 11), (67531, 5), (51961, 11), (67513, 1), (66561, 18), (59657, 11), (66423, 18), (41965, 11), (6941, 1), (66686, 11), (66723, 4), (66791, 4), (67751, 4), (67467, 5), (67016, 12), (667, 1), (66237, 19), (11781, 1), (67561, 4), (55712, 18), (11020, 11), (66806, 11), (16639, 1), (8412, 11), (67411, 10), (66858, 19), (67773, 4), (27763, 12), (67175, 21), (51783, 1), (8691, 22), (51243, 18), (66176, 19), (67313, 12), (66505, 1), (67665, 11), (66594, 4), (67811, 1), (22296, 11), (4097, 11), (67706, 4), (16512, 11), (66991, 12), (67442, 4), (66434, 22), (67841, 11), (67226, 19), (633, 1), (7393, 12), (67000, 4), (66151, 5), (67598, 6), (66387, 4), (11738, 21), (67178, 21), (8035, 6), (66207, 21), (65595, 11), (2189, 11), (67421, 4), (16542, 11), (66317, 4), (24177, 10), (66428, 21), (66420, 22), (67621, 1), (38737, 4), (65150, 11), (66236, 1), (47514, 3), (32219, 6), (67527, 7), (61579, 6), (4082, 11), (67829, 11), (30358, 4), (67353, 11), (66765, 4), (856, 11), (66770, 11), (67641, 1), (37860, 4), (33500, 6), (67159, 3), (2504, 11), (66690, 4), (67102, 12), (14404, 10), (5758, 21), (67091, 4), (67408, 11), (6483, 19), (66270, 1), (22276, 11), (1407, 11), (10195, 12), (66218, 11), (23780, 12), (66280, 18), (16224, 3), (66277, 8), (66404, 8), (44570, 10), (47445, 3), (23997, 8), (66873, 4), (67202, 4), (66678, 11), (2956, 18), (66186, 18), (67294, 3), (67362, 11), (66452, 4), (4099, 11), (67760, 4), (67053, 3), (59738, 4), (66970, 4), (976, 1), (67512, 4), (67449, 15), (38870, 6), (67696, 4), (6444, 12), (41920, 11), (66988, 4), (13833, 22), (66564, 4), (66488, 4), (66974, 4), (67844, 11), (67781, 4), (36439, 8), (66987, 21), (65427, 11), (42139, 3), (66439, 17), (66548, 4), (67704, 4), (67502, 5), (67491, 1), (22291, 11), (67260, 18), (2459, 11), (66986, 12), (66435, 4), (61589, 12), (66967, 4), (67079, 8), (67186, 12), (67256, 3), (66710, 4), (66749, 11), (67127, 10), (67574, 4), (60006, 7), (57712, 8), (66689, 4), (67167, 11), (16647, 12), (67009, 18), (16537, 11), (1085, 1), (66495, 4), (18521, 3), (25799, 19), (64368, 19), (33881, 3), (66642, 12), (65126, 11), (67794, 4), (13500, 11), (67143, 1), (66584, 4), (445, 10), (67206, 12), (29446, 4), (11985, 11), (67273, 11), (67225, 8), (65828, 11), (6341, 11), (2321, 11), (67580, 6), (67572, 6), (26310, 3), (66613, 4), (66197, 21), (9080, 6), (66315, 4), (67314, 11), (67046, 1), (49855, 11), (14704, 12), (67361, 5), (67251, 3), (67181, 21), (66637, 12), (50426, 8), (32239, 3), (27765, 1), (65557, 11), (66920, 4), (67044, 7), (16807, 1), (67546, 22), (31542, 8), (10364, 11), (67688, 4), (66460, 11), (66511, 18), (64954, 11), (67088, 19), (280, 1), (66631, 4), (67776, 4), (39213, 8), (67196, 19), (10445, 3), (66462, 12), (66349, 4), (67259, 11), (5445, 12), (10656, 19), (66274, 8), (66285, 4), (67293, 9), (2450, 11), (42757, 3), (34799, 11), (67188, 4), (57336, 10), (19111, 17), (1195, 1), (66650, 11), (67724, 4), (28853, 17), (3258, 12), (23241, 11), (67816, 1), (66376, 18), (56286, 18), (66851, 4), (66235, 8), (65553, 11), (66696, 18), (65937, 11), (54360, 7), (67131, 4), (3589, 6), (12226, 6), (66526, 4), (67218, 4), (66767, 12), (8886, 12), (18150, 3), (67394, 11), (66821, 4), (65968, 11), (16553, 11), (67128, 10), (66551, 9), (617, 11), (66870, 17), (1058, 1), (66663, 8), (12495, 6), (66815, 4), (67501, 22), (67125, 19), (67187, 17), (67342, 5), (34530, 1), (66481, 19), (11171, 6), (66489, 11), (2983, 10), (26261, 6), (66565, 4), (66944, 12), (11729, 4), (890, 1), (67379, 12), (151, 11), (66810, 18), (50030, 12), (67476, 12), (11704, 4), (66402, 4), (66253, 8), (3822, 11), (32770, 4), (17397, 11), (53369, 3), (9370, 10), (66720, 11), (17895, 6), (67367, 12), (67725, 4), (52980, 11)], [(241961, 10), (873, 10), (241947, 10), (241959, 10), (241955, 10), (171658, 10), (12621, 10), (94099, 10), (241954, 10), (72759, 10), (90362, 10), (56702, 10), (241952, 11), (241948, 10), (153310, 10), (39782, 10), (177183, 10), (241950, 10), (241957, 10), (28702, 10), (3907, 10), (241958, 10), (19915, 10), (241953, 10), (8749, 10), (25521, 10), (19892, 10), (241956, 10), (241963, 10), (2589, 10), (177025, 11), (56650, 10)], [(72854, 4), (23814, 9), (151551, 15), (55226, 9), (98601, 15), (33348, 7), (242466, 7), (64189, 15), (141017, 12), (115903, 21), (9528, 7), (242467, 7), (63752, 12), (5309, 7), (242464, 7), (242468, 15), (242465, 7)], [(534, 1), (4472, 1), (3961, 1), (2544, 1), (89688, 12), (9254, 10), (79470, 10), (12698, 1), (2975, 11), (157564, 15), (51, 1), (88801, 11), (45233, 10)], [(229571, 17), (229570, 16), (45237, 5), (123650, 17), (97245, 17), (2983, 10), (117850, 10)], [(29101, 3), (158175, 16), (34551, 3), (885, 3), (61720, 8), (1328, 22), (27866, 12), (227133, 12), (67405, 12), (12913, 3), (10565, 12), (115115, 3), (227132, 3), (187932, 3), (71201, 3)], [(7068, 1), (14433, 11), (3538, 11), (16967, 14), (17891, 14), (17890, 3), (17883, 14), (9291, 10), (7246, 5), (4036, 2), (814, 2), (192, 14), (17879, 6), (17838, 7), (10583, 14), (17847, 10), (374, 1), (3700, 17), (4062, 19), (1078, 10), (17875, 10), (451, 14), (17877, 10), (756, 7), (4886, 14), (17881, 16), (17869, 6), (528, 6), (17867, 10), (17872, 10), (3877, 15), (17855, 6), (16996, 10), (17871, 10), (7228, 10), (3412, 6), (17884, 14), (17901, 15), (10235, 14), (17882, 14), (17899, 7), (17560, 14), (17895, 6), (17904, 2), (17849, 10), (17903, 7), (7403, 10), (2605, 10), (17844, 2), (856, 11), (17892, 14), (17848, 10), (17856, 6), (684, 11), (10932, 14), (17858, 10), (1273, 10), (17886, 16), (17900, 19), (17896, 2), (4495, 10), (11031, 6), (17864, 10), (17862, 10), (17846, 10), (442, 11), (181, 14), (10503, 14), (669, 14), (17852, 10), (17850, 18), (17897, 4), (697, 11), (4428, 14), (17860, 10), (17889, 17), (7226, 10), (4377, 14), (4472, 1), (17839, 9), (17851, 10), (17870, 10), (4908, 14), (17880, 6), (10027, 7), (15663, 10), (10489, 6), (387, 14), (10640, 10), (4359, 11), (5245, 16), (17874, 10), (17853, 6), (17876, 10), (17863, 10), (11985, 11), (306, 17), (2598, 14), (4904, 14), (2517, 11), (17887, 14), (17894, 6), (17854, 6), (5422, 14), (17888, 14), (4891, 17), (5874, 9), (7227, 10), (386, 14), (17845, 11), (17665, 10), (17878, 6), (183, 10), (17861, 10), (589, 7)]]\n",
      "[(3077, 2), (3265, 2), (3699, 2), (5739, 2), (6799, 2), (7261, 2), (7618, 2), (10271, 2), (10406, 2), (12075, 2), (13040, 2), (13451, 2), (14239, 2), (14804, 2), (14937, 2), (16818, 2), (18536, 2), (18798, 2), (19006, 2), (19430, 2), (19632, 2), (20489, 2), (21152, 2), (21315, 2), (21792, 2), (21801, 2), (21891, 2), (22012, 2)]\n"
     ]
    }
   ],
   "source": [
    "for i in train_data:\n",
    "    for j in i:\n",
    "        print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, collate_fn = collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = False, collate_fn = collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5704"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21936, 21419, 10779, 11366,   767,  1453,  9327,  2678, 19220,  4563,\n",
      "         8204, 17856,  4370,  5267, 20786,  2585, 18463, 13737, 15663,  3637,\n",
      "        19431, 10256,  2825,  7054, 10466,  6764, 15974,  2198, 10861,  4269,\n",
      "        11289, 18789,  7246,  8554, 13882,  8813,  3077,  3252,   791,  6138,\n",
      "        12346, 20386, 15203, 20152, 10225,  7246,  7780, 15686,  7859,  5086,\n",
      "        18837, 20406, 15394,  7944,  8601, 21266, 15434, 13658,  3286, 15998,\n",
      "        17718, 16059,  4399, 19505,  5642, 13588, 16742,  5711, 10155,  1368,\n",
      "          974,  9765,  4124,  4586,  4591, 20461, 10608, 15406, 19493, 21864,\n",
      "         9092, 13569, 15573,  9952, 10409, 21862, 11349,  5530,  5419,  8014,\n",
      "         9555, 13310,  2318,  6492,  6813,  7136, 14933, 13660,  1502, 16595,\n",
      "         4492, 18525, 11739, 12282,  3301,  5161, 20598, 19793, 19505, 13728,\n",
      "         3251,  3746, 12382,  4432, 13313, 22164, 20461,  5278, 11081, 15417,\n",
      "        20373, 12270, 15043, 18404, 18942, 19537,  9880,  9304])\n",
      "tensor([294340, 289747,  68014,  37029,  21318,  35629, 165164,  64050,  96547,\n",
      "          5272,   1960, 111998,  91060, 106062,  94118,  62053,   9265,  88488,\n",
      "         75343,  82926,    923, 124321,  68743,   3412, 176680,  14278,  17845,\n",
      "         43537, 180372,  91712,  53936,  23344, 119258, 123001, 101929, 156840,\n",
      "         72644,  17471,  12705, 119398,   4987,  62506,   4777, 279560,  31360,\n",
      "         16470,  10311,  35532, 146763, 102993,  16277,  21803,  47201,     58,\n",
      "         41765,  60624, 170785, 212316,  76923, 239909,  28812, 241104,  92989,\n",
      "        273841,  10023,  17771,    885, 114551, 173379,   1637,  26164,    927,\n",
      "         12669,  56311,  37876,   3090,   4282,  34119,  19428, 278687,    450,\n",
      "        209865, 157908, 171466,   4242, 293455,   3458, 110993,    900,   3366,\n",
      "         10028,  22846,  55634,    888,  76098,      7, 226273,  55940,  37250,\n",
      "        188968,  94609, 262311,  31257,  74764,  37413,  65096,  31254,     78,\n",
      "         61539, 213689,  22539,  84076, 197512,  93536,  22940,  26399, 225355,\n",
      "        106361, 168474, 230441, 216248,     51,  18686, 134163,   7068,  88481,\n",
      "           385,     16])\n",
      "tensor([ 8., 19.,  8., 11.,  3.,  7.,  4.,  8.,  5., 10.,  1., 15.,  5., 11.,\n",
      "         2., 18., 10., 19., 19.,  7.,  9.,  8., 17.,  6.,  4.,  3., 11.,  5.,\n",
      "         4.,  5.,  5.,  3.,  4.,  2., 18., 18.,  2.,  4.,  3.,  4., 19.,  5.,\n",
      "         7.,  8.,  9.,  3.,  3., 15., 19.,  3.,  8., 19.,  5.,  5.,  2., 18.,\n",
      "        19.,  4.,  5.,  4.,  4., 11.,  7.,  1.,  3.,  9.,  3.,  5.,  5.,  2.,\n",
      "        10.,  9.,  4.,  5.,  3., 18., 18., 18.,  7.,  7., 11., 19., 18., 18.,\n",
      "        11.,  3., 10.,  4.,  2.,  2.,  3.,  4.,  7., 10., 19.,  3.,  4., 19.,\n",
      "         5., 15.,  4., 11., 19.,  3.,  5., 11., 19., 17.,  8., 19.,  5.,  4.,\n",
      "        19., 21.,  3.,  5.,  3.,  5.,  5.,  8., 18.,  1., 18.,  3.,  1., 17.,\n",
      "         3.,  5.])\n",
      "tensor([[[294338,     18],\n",
      "         [264754,      8],\n",
      "         [294341,     10],\n",
      "         ...,\n",
      "         [     0,      0],\n",
      "         [     0,      0],\n",
      "         [     0,      0]],\n",
      "\n",
      "        [[289851,     19],\n",
      "         [253223,     19],\n",
      "         [289870,     19],\n",
      "         ...,\n",
      "         [289803,     19],\n",
      "         [289791,     18],\n",
      "         [282147,     19]],\n",
      "\n",
      "        [[ 33522,     18],\n",
      "         [155312,     18],\n",
      "         [  5785,     10],\n",
      "         ...,\n",
      "         [     0,      0],\n",
      "         [     0,      0],\n",
      "         [     0,      0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 78059,      5],\n",
      "         [ 49416,     17],\n",
      "         [ 38659,     11],\n",
      "         ...,\n",
      "         [274150,     10],\n",
      "         [  9801,      7],\n",
      "         [ 26659,     20]],\n",
      "\n",
      "        [[170979,      3],\n",
      "         [ 21952,      3],\n",
      "         [ 34440,      5],\n",
      "         ...,\n",
      "         [ 76428,      3],\n",
      "         [170980,      3],\n",
      "         [     0,      0]],\n",
      "\n",
      "        [[ 89339,      5],\n",
      "         [    11,      3],\n",
      "         [ 69457,      5],\n",
      "         ...,\n",
      "         [   492,      3],\n",
      "         [ 34440,      5],\n",
      "         [148197,      5]]])\n",
      "tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 2110, 19234, 17348,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [12895, 20636, 16886,  ..., 19542, 15249,  1896]])\n",
      "tensor([[[[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]]],\n",
      "\n",
      "\n",
      "        [[[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]]],\n",
      "\n",
      "\n",
      "        [[[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[  1586,     19],\n",
      "          [   122,      1],\n",
      "          [  1547,     18],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[ 72967,      5],\n",
      "          [  7965,      2],\n",
      "          [ 23288,      2],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[  6101,      3],\n",
      "          [  9101,      3],\n",
      "          [ 23323,      3],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]]],\n",
      "\n",
      "\n",
      "        [[[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]]],\n",
      "\n",
      "\n",
      "        [[[202523,      1],\n",
      "          [202531,      8],\n",
      "          [ 11267,     19],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[ 21276,      6],\n",
      "          [  1207,      1],\n",
      "          [   649,      3],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[ 28945,      7],\n",
      "          [248529,     20],\n",
      "          [170584,     15],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  8876,      5],\n",
      "          [  3564,      5],\n",
      "          [ 45233,     10],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[ 19668,      6],\n",
      "          [115274,     19],\n",
      "          [ 51246,      4],\n",
      "          ...,\n",
      "          [     0,      0],\n",
      "          [     0,      0],\n",
      "          [     0,      0]],\n",
      "\n",
      "         [[ 44992,     10],\n",
      "          [ 45040,     11],\n",
      "          [ 45010,     15],\n",
      "          ...,\n",
      "          [  1533,     15],\n",
      "          [ 44988,      9],\n",
      "          [ 23806,     11]]]])\n",
      "tensor([[[21936,     8],\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        [[21419,    19],\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        [[ 2766,     8],\n",
      "         [ 6144,     8],\n",
      "         [ 6754,     8],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4030,    17],\n",
      "         [ 4780,    17],\n",
      "         [ 5509,    17],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        [[  256,     3],\n",
      "         [16343,     3],\n",
      "         [ 4204,     3],\n",
      "         ...,\n",
      "         [ 3828,     3],\n",
      "         [16689,     3],\n",
      "         [11700,     3]],\n",
      "\n",
      "        [[12432,     5],\n",
      "         [16118,     5],\n",
      "         [ 6769,     5],\n",
      "         ...,\n",
      "         [14620,     5],\n",
      "         [17769,     5],\n",
      "         [19061,     5]]])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    for j in i:\n",
    "        print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model and set up training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphRec(user_count+1, item_count+1, rate_count+1, embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m---> 52\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRMSprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\rmsprop.py:46\u001b[0m, in \u001b[0;36mRMSprop.__init__\u001b[1;34m(self, params, lr, alpha, eps, weight_decay, momentum, centered, foreach, maximize, differentiable)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid alpha value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     36\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     37\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:62\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     CheckFunctionManager,\n\u001b[0;32m     58\u001b[0m     get_and_maybe_log_recompilation_reason,\n\u001b[0;32m     59\u001b[0m     GuardedCode,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputGraph\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreplay_record\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutionRecord\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InstructionTranslator, SpeculationLog\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonReferenceAnalysis\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweak\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeakTensorKeyDictionary\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging, variables\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompiledFn, CompilerFn\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     create_call_function,\n\u001b[0;32m     43\u001b[0m     create_instruction,\n\u001b[0;32m     44\u001b[0m     Instruction,\n\u001b[0;32m     45\u001b[0m     unique_id,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\__init__.py:68\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NNModuleVariable, UnspecializedNNModuleVariable\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     FakeItemVariable,\n\u001b[0;32m     63\u001b[0m     NumpyNdarrayVariable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     UnspecializedPythonVariable,\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     TorchCtxManagerClassVariable,\n\u001b[0;32m     70\u001b[0m     TorchInGraphFunctionVariable,\n\u001b[0;32m     71\u001b[0m     TorchVariable,\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_defined\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserDefinedClassVariable, UserDefinedObjectVariable\n\u001b[0;32m     75\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionContextVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWithExitFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\torch.py:95\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m     80\u001b[0m     constant_fold_functions\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m     81\u001b[0m         [\n\u001b[0;32m     82\u001b[0m             torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m         ]\n\u001b[0;32m     86\u001b[0m     )\n\u001b[0;32m     89\u001b[0m tracing_state_functions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39mis_fx_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     94\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mis_in_onnx_export: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal_utils\u001b[49m\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m }\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseTorchVariable\u001b[39;00m(VariableTracker):\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"common base for all torch.* functions, classes, modules and other things\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Disable dynamo by setting environment variable\n",
    "os.environ[\"PYTORCH_DYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "# Assuming you have defined your model architecture\n",
    "class SocialRecommendationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SocialRecommendationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Assuming you have defined your dataset class and DataLoader\n",
    "class SocialRecommendationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Define your model, dataset, and other necessary components\n",
    "input_size = 100  # Example input size\n",
    "hidden_size = 50  # Example hidden size\n",
    "output_size = 1  # Example output size\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Example data, replace with your actual dataset\n",
    "# Assuming 'data' is a list of tuples where each tuple contains input and target data\n",
    "data = [(torch.randn(input_size), torch.randn(output_size)) for _ in range(1000)]\n",
    "\n",
    "model = SocialRecommendationModel(input_size, hidden_size, output_size)\n",
    "dataset = SocialRecommendationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_data in dataloader:\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m---> 52\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRMSprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\rmsprop.py:46\u001b[0m, in \u001b[0;36mRMSprop.__init__\u001b[1;34m(self, params, lr, alpha, eps, weight_decay, momentum, centered, foreach, maximize, differentiable)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid alpha value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     36\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     37\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:62\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     CheckFunctionManager,\n\u001b[0;32m     58\u001b[0m     get_and_maybe_log_recompilation_reason,\n\u001b[0;32m     59\u001b[0m     GuardedCode,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputGraph\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreplay_record\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutionRecord\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InstructionTranslator, SpeculationLog\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonReferenceAnalysis\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweak\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeakTensorKeyDictionary\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging, variables\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompiledFn, CompilerFn\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     create_call_function,\n\u001b[0;32m     43\u001b[0m     create_instruction,\n\u001b[0;32m     44\u001b[0m     Instruction,\n\u001b[0;32m     45\u001b[0m     unique_id,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\__init__.py:68\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NNModuleVariable, UnspecializedNNModuleVariable\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     FakeItemVariable,\n\u001b[0;32m     63\u001b[0m     NumpyNdarrayVariable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     UnspecializedPythonVariable,\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     TorchCtxManagerClassVariable,\n\u001b[0;32m     70\u001b[0m     TorchInGraphFunctionVariable,\n\u001b[0;32m     71\u001b[0m     TorchVariable,\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_defined\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserDefinedClassVariable, UserDefinedObjectVariable\n\u001b[0;32m     75\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionContextVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWithExitFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\torch.py:95\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m     80\u001b[0m     constant_fold_functions\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m     81\u001b[0m         [\n\u001b[0;32m     82\u001b[0m             torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m         ]\n\u001b[0;32m     86\u001b[0m     )\n\u001b[0;32m     89\u001b[0m tracing_state_functions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39mis_fx_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     94\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mis_in_onnx_export: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal_utils\u001b[49m\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m }\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseTorchVariable\u001b[39;00m(VariableTracker):\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"common base for all torch.* functions, classes, modules and other things\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Disable dynamo by setting environment variable\n",
    "os.environ[\"PYTORCH_DYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "# Assuming you have defined your model architecture\n",
    "class SocialRecommendationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SocialRecommendationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Assuming you have defined your dataset class and DataLoader\n",
    "class SocialRecommendationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Define your model, dataset, and other necessary components\n",
    "input_size = 100  # Example input size\n",
    "hidden_size = 50  # Example hidden size\n",
    "output_size = 1  # Example output size\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Example data, replace with your actual dataset\n",
    "# Assuming 'data' is a list of tuples where each tuple contains input and target data\n",
    "data = [(torch.randn(input_size), torch.randn(output_size)) for _ in range(1000)]\n",
    "\n",
    "model = SocialRecommendationModel(input_size, hidden_size, output_size)\n",
    "dataset = SocialRecommendationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_data in dataloader:\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m---> 48\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRMSprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\rmsprop.py:46\u001b[0m, in \u001b[0;36mRMSprop.__init__\u001b[1;34m(self, params, lr, alpha, eps, weight_decay, momentum, centered, foreach, maximize, differentiable)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid alpha value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     36\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     37\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:62\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     CheckFunctionManager,\n\u001b[0;32m     58\u001b[0m     get_and_maybe_log_recompilation_reason,\n\u001b[0;32m     59\u001b[0m     GuardedCode,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputGraph\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreplay_record\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutionRecord\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InstructionTranslator, SpeculationLog\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonReferenceAnalysis\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweak\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeakTensorKeyDictionary\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging, variables\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompiledFn, CompilerFn\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     create_call_function,\n\u001b[0;32m     43\u001b[0m     create_instruction,\n\u001b[0;32m     44\u001b[0m     Instruction,\n\u001b[0;32m     45\u001b[0m     unique_id,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\__init__.py:68\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NNModuleVariable, UnspecializedNNModuleVariable\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     FakeItemVariable,\n\u001b[0;32m     63\u001b[0m     NumpyNdarrayVariable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     UnspecializedPythonVariable,\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     TorchCtxManagerClassVariable,\n\u001b[0;32m     70\u001b[0m     TorchInGraphFunctionVariable,\n\u001b[0;32m     71\u001b[0m     TorchVariable,\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_defined\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserDefinedClassVariable, UserDefinedObjectVariable\n\u001b[0;32m     75\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionContextVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWithExitFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\torch.py:95\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m     80\u001b[0m     constant_fold_functions\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m     81\u001b[0m         [\n\u001b[0;32m     82\u001b[0m             torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m         ]\n\u001b[0;32m     86\u001b[0m     )\n\u001b[0;32m     89\u001b[0m tracing_state_functions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39mis_fx_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     94\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mis_in_onnx_export: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal_utils\u001b[49m\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m }\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseTorchVariable\u001b[39;00m(VariableTracker):\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"common base for all torch.* functions, classes, modules and other things\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have defined your model architecture\n",
    "class SocialRecommendationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SocialRecommendationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Assuming you have defined your dataset class and DataLoader\n",
    "class SocialRecommendationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Define your model, dataset, and other necessary components\n",
    "input_size = 100  # Example input size\n",
    "hidden_size = 50  # Example hidden size\n",
    "output_size = 1  # Example output size\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Example data, replace with your actual dataset\n",
    "# Assuming 'data' is a list of tuples where each tuple contains input and target data\n",
    "data = [(torch.randn(input_size), torch.randn(output_size)) for _ in range(1000)]\n",
    "\n",
    "model = SocialRecommendationModel(input_size, hidden_size, output_size)\n",
    "dataset = SocialRecommendationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_data in dataloader:\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_disable_dynamo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Disable dynamo\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_disable_dynamo\u001b[49m()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming you have defined your model architecture\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSocialRecommendationModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_disable_dynamo'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Disable dynamo\n",
    "torch._C._disable_dynamo()\n",
    "\n",
    "# Assuming you have defined your model architecture\n",
    "class SocialRecommendationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SocialRecommendationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Assuming you have defined your dataset class and DataLoader\n",
    "class SocialRecommendationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Define your model, dataset, and other necessary components\n",
    "input_size = 100  # Example input size\n",
    "hidden_size = 50  # Example hidden size\n",
    "output_size = 1  # Example output size\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Example data, replace with your actual dataset\n",
    "# Assuming 'data' is a list of tuples where each tuple contains input and target data\n",
    "data = [(torch.randn(input_size), torch.randn(output_size)) for _ in range(1000)]\n",
    "\n",
    "model = SocialRecommendationModel(input_size, hidden_size, output_size)\n",
    "dataset = SocialRecommendationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_data in dataloader:\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5704 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5704 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m u_users_items \u001b[38;5;241m=\u001b[39m u_users_items\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m i_users \u001b[38;5;241m=\u001b[39m i_users\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 15\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(uids, iids, u_items, u_users, u_users_items, i_users)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Training step\n",
    "    model.train()\n",
    "    s_loss = 0\n",
    "    for i, (uids, iids, labels, u_items, u_users, u_users_items, i_users) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        uids = uids.to(device)\n",
    "        iids = iids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        u_items = u_items.to(device)\n",
    "        u_users = u_users.to(device)\n",
    "        u_users_items = u_users_items.to(device)\n",
    "        i_users = i_users.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(uids, iids, u_items, u_users, u_users_items, i_users)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        s_loss += loss_val\n",
    "\n",
    "        iter_num = epoch * len(train_loader) + i + 1\n",
    "\n",
    "    # Validate step\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for uids, iids, labels, u_items, u_users, u_users_items, i_users in tqdm(valid_loader):\n",
    "            uids = uids.to(device)\n",
    "            iids = iids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            u_items = u_items.to(device)\n",
    "            u_users = u_users.to(device)\n",
    "            u_users_items = u_users_items.to(device)\n",
    "            i_users = i_users.to(device)\n",
    "            preds = model(uids, iids, u_items, u_users, u_users_items, i_users)\n",
    "            error = torch.abs(preds.squeeze(1) - labels)\n",
    "            errors.extend(error.data.cpu().numpy().tolist())\n",
    "    \n",
    "    mae = np.mean(errors)\n",
    "    rmse = np.sqrt(np.mean(np.power(errors, 2)))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    ckpt_dict = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(ckpt_dict, 'trained models epinions/latest_checkpoint.pth')\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_mae = mae\n",
    "    elif mae < best_mae:\n",
    "        best_mae = mae\n",
    "        torch.save(ckpt_dict, 'trained models epinions/best_checkpoint_{}.pth'.format(embed_dim))\n",
    "\n",
    "    print('Epoch {} validation: MAE: {:.4f}, RMSE: {:.4f}, Best MAE: {:.4f}'.format(epoch+1, mae, rmse, best_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained models epinions/best_checkpoint_64.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrained models epinions/best_checkpoint_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GraphRec(user_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, item_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, rate_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, embed_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained models epinions/best_checkpoint_64.pth'"
     ]
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "checkpoint = torch.load('trained models epinions/best_checkpoint_{}.pth'.format(embed_dim))\n",
    "model = GraphRec(user_count+1, item_count+1, rate_count+1, embed_dim).to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/713 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SocialRecommendationModel.forward() takes 2 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m u_users_items \u001b[38;5;241m=\u001b[39m u_users_items\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m i_users \u001b[38;5;241m=\u001b[39m i_users\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_users_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_users\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m error \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(preds\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m labels)\n\u001b[0;32m     14\u001b[0m test_errors\u001b[38;5;241m.\u001b[39mextend(error\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ch.sathwika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: SocialRecommendationModel.forward() takes 2 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_errors = []\n",
    "with torch.no_grad():\n",
    "    for uids, iids, labels, u_items, u_users, u_users_items, i_users in tqdm(test_loader):\n",
    "        uids = uids.to(device)\n",
    "        iids = iids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        u_items = u_items.to(device)\n",
    "        u_users = u_users.to(device)\n",
    "        u_users_items = u_users_items.to(device)\n",
    "        i_users = i_users.to(device)\n",
    "        preds = model(uids, iids, u_items, u_users, u_users_items, i_users)\n",
    "        error = torch.abs(preds.squeeze(1) - labels)\n",
    "        test_errors.extend(error.data.cpu().numpy().tolist())\n",
    "\n",
    "test_mae = np.mean(test_errors)\n",
    "test_rmse = np.sqrt(np.mean(np.power(test_errors, 2)))\n",
    "print('Test: MAE: {:.4f}, RMSE: {:.4f}'.format(test_mae, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec7d137c6e0bfe5aec1849c5c512dd0bf44cf2eb2fae9fc2de49724729b3d6c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('torch1.7': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
